<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>performance on A Place for Poor Examples</title>
    <link>https://blog.mattblair.co/tags/performance/</link>
    <description>Recent content in performance on A Place for Poor Examples</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en</language>
    <lastBuildDate>Wed, 21 Nov 2018 00:00:00 +0000</lastBuildDate><atom:link href="https://blog.mattblair.co/tags/performance/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>ElasticSearch Perf - Highlighter Edition</title>
      <link>https://blog.mattblair.co/blog/20181121-elasticsearch-perf-work/</link>
      <pubDate>Wed, 21 Nov 2018 00:00:00 +0000</pubDate>
      
      <guid>https://blog.mattblair.co/blog/20181121-elasticsearch-perf-work/</guid>
      <description>Wanted to share the results of the work we&amp;rsquo;ve been doing to improve performance and stabilize our ES cluster.
Short Version: Changing sharding and &amp;ldquo;highlighting&amp;rdquo; strategies when doing FreeText searches dramatically impacts performance.
Long Version: We&amp;rsquo;ve been load testing various aspects of search to pinpoint features we use that create large amounts of load on the system. Our initial research pointed at normal searches and freetext searches as being problematic.</description>
    </item>
    
    <item>
      <title>Moment.js alternatives</title>
      <link>https://blog.mattblair.co/blog/20180908-moment-js-alternatives/</link>
      <pubDate>Sat, 08 Sep 2018 00:00:00 +0000</pubDate>
      
      <guid>https://blog.mattblair.co/blog/20180908-moment-js-alternatives/</guid>
      <description>If you&amp;rsquo;re looking for moment.js alternatives, I&amp;rsquo;d recommend reading this article about smaller, lighter-weight moment.js alternatives.</description>
    </item>
    
    <item>
      <title>Moment.js instantiation slowness</title>
      <link>https://blog.mattblair.co/blog/20170504-moment-js-instantiation-slowness/</link>
      <pubDate>Thu, 04 May 2017 00:00:00 +0000</pubDate>
      
      <guid>https://blog.mattblair.co/blog/20170504-moment-js-instantiation-slowness/</guid>
      <description>Was doing some test speedup/performance improvement work recently on the search API and found out something; the moment.js library takes around 100 microseconds (or .1 milliseconds) to create a new instance.
Why is 100 microseconds a big deal?
If you&amp;rsquo;re processing:
 100 records Where each record has 7 date fields Then you&amp;rsquo;ve created 70,000 microseconds of work Or 70 milliseconds of processing delay.  By doing some memoization of date formatting in our API, we&amp;rsquo;ve seen these performance improvements:</description>
    </item>
    
    <item>
      <title>Performance Improvement via node 4 to node 6</title>
      <link>https://blog.mattblair.co/blog/20161109-performance-improvement-via-babel-tranpilation-removal/</link>
      <pubDate>Wed, 09 Nov 2016 00:00:00 +0000</pubDate>
      
      <guid>https://blog.mattblair.co/blog/20161109-performance-improvement-via-babel-tranpilation-removal/</guid>
      <description>My team at work recently upgraded our codebase to use to node 6.9, as node 6 has recently gone to LTS.
In the picture below, the 1st line is the upgrade from node 4 to node 6, and the corresponding flattening of memory usage vs. load.
The 2nd line is our removal of redis connection queueing in the application.
All of all, the memory consumption of our application is now averaging around 150MB, down from a high of 1GB!</description>
    </item>
    
    <item>
      <title>Superagent/Request Memory Leaks</title>
      <link>https://blog.mattblair.co/blog/20161017-superagent-request-memory-leaks/</link>
      <pubDate>Mon, 17 Oct 2016 00:00:00 +0000</pubDate>
      
      <guid>https://blog.mattblair.co/blog/20161017-superagent-request-memory-leaks/</guid>
      <description>Superagent/Request Memory Leaks The last several weeks Thomas Hunter and myself have spent some of our nights and weekends trying to track down memory leaks in an API we both work on.
We were seeing a distinct pattern, that when the API was put under a certain amount of load, that we would start slowly bleeding memory.
We&amp;rsquo;ve found three results:
  Superagent, when put under a certain threshold of load and then connections timeout, can leak memory.</description>
    </item>
    
  </channel>
</rss>
